{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 12595047463170636152\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 7061687501\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 13243204908062269976\n",
      "physical_device_desc: \"device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import random\n",
    "import os, shutil\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "# from keras_applications.resnet import ResNet152\n",
    "# from keras.applications.inception_v3 import InceptionV3\n",
    "# from keras.applications.inception_v3 import preprocess_input\n",
    "# from keras_efficientnets.efficientnet import EfficientNetB3\n",
    "# from keras_efficientnets.efficientnet import preprocess_input\n",
    "\n",
    "from keras.preprocessing import image\n",
    "\n",
    "from keras.layers import GlobalAveragePooling2D, Dense, Dropout, Flatten,Input, concatenate, Conv2D, BatchNormalization, MaxPooling2D, Activation\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras import optimizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import backend, layers, models,utils\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "from sklearn.model_selection import train_test_split\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为每个版本设编号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_width, im_height = 224, 224\n",
    "lr = 0.000001\n",
    "batch_size = 32\n",
    "epochs = 40\n",
    "valid_size = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "切分训练集和验证集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('clean_data.txt', 'rb') as cd:\n",
    "    clean_data = pickle.load(cd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19894 4974 19894 4974\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_names</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cat.0.jpg</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cat.1.jpg</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cat.10.jpg</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cat.100.jpg</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cat.1000.jpg</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     file_names labels\n",
       "0     cat.0.jpg    cat\n",
       "1     cat.1.jpg    cat\n",
       "2    cat.10.jpg    cat\n",
       "3   cat.100.jpg    cat\n",
       "4  cat.1000.jpg    cat"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = './train/'\n",
    "\n",
    "with open('data_set.txt', 'rb') as ds:\n",
    "    data_set = pickle.load(ds)\n",
    "file_df = pd.DataFrame({'file_names': clean_data, 'labels': [x.split('.')[0] for x in clean_data]})\n",
    "file_train, file_valid, label_train, label_valid = train_test_split(file_df['file_names'], file_df['labels'],\n",
    "                                                                    test_size = valid_size, random_state = 123)\n",
    "file_valid = file_valid.tolist()\n",
    "file_train = file_train.tolist()\n",
    "print(len(file_train), len(file_valid), len(label_train), len(label_valid))\n",
    "file_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = {'file_train': file_train, 'label_train': label_train, 'file_valid': file_valid, 'label_valid': label_valid}\n",
    "with open('data_set.txt', 'wb') as ds:\n",
    "    pickle.dump(data_set, ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "检查训练集和验证集样本分布。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2551</th>\n",
       "      <td>cat.12312.jpg</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9057</th>\n",
       "      <td>cat.6969.jpg</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13114</th>\n",
       "      <td>dog.10654.jpg</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9469</th>\n",
       "      <td>cat.7340.jpg</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14249</th>\n",
       "      <td>dog.11679.jpg</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   X    Y\n",
       "2551   cat.12312.jpg  cat\n",
       "9057    cat.6969.jpg  cat\n",
       "13114  dog.10654.jpg  dog\n",
       "9469    cat.7340.jpg  cat\n",
       "14249  dog.11679.jpg  dog"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataf = {x: pd.DataFrame({'X': data_set['file_{}'.format(x)],\n",
    "                          'Y'.format(x): data_set['label_{}'.format(x)]}) for x in ['train', 'valid']}\n",
    "dataf['train'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据增强。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = {x: image.ImageDataGenerator(preprocessing_function=preprocess_input) for x in dataf.keys()}\n",
    "# datagen = image.ImageDataGenerator(preprocessing_function=preprocess_input, validation_split=valid_size)\n",
    "datagen['train'].horizontal_flip = True\n",
    "datagen['train'].shear_range = 0.2\n",
    "datagen['train'].zoom_range = [0.8, 1.2]\n",
    "# datagen = image.ImageDataGenerator(preprocessing_function=preprocess_input,\n",
    "#                                   horizontal_flip=True,\n",
    "#                                   shear_range=0.2,\n",
    "#                                   zoom_range=[0.8, 1.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19894 validated image filenames belonging to 2 classes.\n",
      "Found 4974 validated image filenames belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "generator = {x: datagen[x].flow_from_dataframe(\n",
    "    dataf[x], x_col = 'X', y_col = 'Y',directory = data_dir.format('train'),\n",
    "    target_size=(im_width, im_height),\n",
    "    batch_size=batch_size,\n",
    "    seed = 123,\n",
    "    class_mode = 'binary',\n",
    "    shuffle = True\n",
    ") for x in datagen.keys()}\n",
    "# generator = datagen.flow_from_dataframe(\n",
    "#     file_df, x_col='file_names', y_col='labels', directory=data_dir.format('train'),\n",
    "#      target_size=(im_width, im_height),\n",
    "#     batch_size=batch_size,\n",
    "#     seed = 123,\n",
    "#     class_mode = 'binary',\n",
    "#     shuffle=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\lucifer\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucifer\\Anaconda3\\envs\\tf\\lib\\site-packages\\keras_applications\\resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\lucifer\\Anaconda3\\envs\\tf\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# model_base = ResNet152(weights = 'imagenet', include_top = False, input_shape = (im_width, im_height, 3),\n",
    "#                            backend = backend, layers = layers, models = models, utils = utils)\n",
    "model_base = ResNet50(weights = 'imagenet', include_top = False, input_shape = (im_width, im_height, 3))\n",
    "model = Sequential()\n",
    "model.add(model_base)\n",
    "model.add(GlobalAveragePooling2D(name = 'm{}_gap_1'.format(ID)))\n",
    "model.add(Dropout(0.75, name = 'm{}_dropout_1'.format(ID)))\n",
    "model.add(Dense(1024, activation = 'relu', name = 'm{}_dense_1'.format(ID)))\n",
    "model.add(Dropout(0.5, name = 'm{}_dropout_2'.format(ID)))\n",
    "model.add(Dense(500, activation = 'relu', name = 'm{}_dense_2'.format(ID)))\n",
    "model.add(Dropout(0.5, name = 'm{}_dropout_3'.format(ID)))\n",
    "model.add(Dense(1, activation='sigmoid', name = 'm{}_dense_3'.format(ID)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # AlexNet\n",
    "# model = Sequential()\n",
    "# #第一段\n",
    "# model.add(Conv2D(filters=96, kernel_size=(11,11),\n",
    "#                  strides=(4,4), padding='valid',\n",
    "#                  input_shape=(im_width,im_height,3),\n",
    "#                  activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(MaxPooling2D(pool_size=(3,3), \n",
    "#                        strides=(2,2), \n",
    "#                        padding='valid'))\n",
    "# #第二段\n",
    "# model.add(Conv2D(filters=256, kernel_size=(5,5), \n",
    "#                  strides=(1,1), padding='same', \n",
    "#                  activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(MaxPooling2D(pool_size=(3,3), \n",
    "#                        strides=(2,2), \n",
    "#                        padding='valid'))\n",
    "# #第三段\n",
    "# model.add(Conv2D(filters=384, kernel_size=(3,3), \n",
    "#                  strides=(1,1), padding='same', \n",
    "#                  activation='relu'))\n",
    "# model.add(Conv2D(filters=384, kernel_size=(3,3), \n",
    "#                  strides=(1,1), padding='same', \n",
    "#                  activation='relu'))\n",
    "# model.add(Conv2D(filters=256, kernel_size=(3,3), \n",
    "#                  strides=(1,1), padding='same', \n",
    "#                  activation='relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(3,3), \n",
    "#                        strides=(2,2), padding='valid'))\n",
    "# #第四段\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(4096, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    " \n",
    "# model.add(Dense(4096, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    " \n",
    "# model.add(Dense(1000, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "\n",
    "# # Output Layer\n",
    "# model.add(Dense(512, activation = 'relu', name = 'm{}_dense_1'.format(ID)))\n",
    "# model.add(Dropout(0.75, name = 'm{}_dropout_1'.format(ID)))\n",
    "# model.add(Dense(256, activation = 'relu', name = 'm{}_dense_2'.format(ID)))\n",
    "# model.add(Dropout(0.75, name = 'm{}_dropout_2'.format(ID)))\n",
    "# model.add(Dense(1))\n",
    "# model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Model)             (None, 7, 7, 2048)        23587712  \n",
      "_________________________________________________________________\n",
      "m30_gap_1 (GlobalAveragePool (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "m30_dropout_1 (Dropout)      (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "m30_dense_1 (Dense)          (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "m30_dropout_2 (Dropout)      (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "m30_dense_2 (Dense)          (None, 500)               512500    \n",
      "_________________________________________________________________\n",
      "m30_dropout_3 (Dropout)      (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "m30_dense_3 (Dense)          (None, 1)                 501       \n",
      "=================================================================\n",
      "Total params: 26,198,889\n",
      "Trainable params: 26,145,769\n",
      "Non-trainable params: 53,120\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable weights after freezing the model_base: 218\n"
     ]
    }
   ],
   "source": [
    "# print('Number of trainable weights bef？or freezing the model_base:', len(model.trainable_weights))\n",
    "# model_base.trainable = False\n",
    "print('Number of trainable weights after freezing the model_base:', len(model.trainable_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\lucifer\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/40\n",
      " - 188s - loss: 1.0727 - acc: 0.5267 - val_loss: 0.5584 - val_acc: 0.7531\n",
      "Epoch 2/40\n",
      " - 179s - loss: 0.9060 - acc: 0.5891 - val_loss: 0.4050 - val_acc: 0.8926\n",
      "Epoch 3/40\n",
      " - 173s - loss: 0.7554 - acc: 0.6549 - val_loss: 0.2884 - val_acc: 0.9419\n",
      "Epoch 4/40\n",
      " - 168s - loss: 0.6061 - acc: 0.7261 - val_loss: 0.1987 - val_acc: 0.9630\n",
      "Epoch 5/40\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "# early_stopping = EarlyStopping(monitor='loss', patience=5)\n",
    "model.compile(loss = \"binary_crossentropy\", optimizer = optimizers.Adam(lr=lr), metrics=[\"accuracy\"])\n",
    "# model.compile(loss = \"binary_crossentropy\", optimizer = optimizers.SGD(lr=lr, momentum = 0.9), metrics=[\"accuracy\"])\n",
    "history = model.fit_generator(generator['train'],\n",
    "                              steps_per_epoch=len(generator['train']),\n",
    "                              epochs=epochs,\n",
    "                              validation_data=generator['valid'],\n",
    "                              validation_steps=len(generator['valid']),\n",
    "                              callbacks=[early_stopping],\n",
    "                              verbose = 2)\n",
    "# history = model.fit_generator(generator,\n",
    "#                               steps_per_epoch=math.ceil(generator.samples / batch_size),\n",
    "#                               epochs=epochs,\n",
    "#                               callbacks=[early_stopping],\n",
    "#                               verbose = 2)\n",
    "\n",
    "model.save('models/model_{}.h5'.format(ID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "x_axis = range(1, len(acc)+1)\n",
    "\n",
    "plt.figure(figsize = [20, 6])\n",
    "plt.subplots_adjust(wspace=0.3)\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(x_axis, acc, 'blue', label='Training Accurarcy')\n",
    "plt.plot(x_axis, val_acc, 'red', label='Validation Accurarcy')\n",
    "plt.title('Training and Validation Accurarcy', fontsize = 20)\n",
    "plt.xlabel('Epoches', fontsize = 16)\n",
    "plt.ylabel('Accuracy', fontsize = 16)\n",
    "plt.legend(fontsize = 14)\n",
    "plt.xticks(ticks=range(0, len(acc)+1, 2),fontsize = 12)\n",
    "plt.yticks(fontsize = 12)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(x_axis, loss, 'blue', label='Training Loss')\n",
    "plt.plot(x_axis, val_loss, 'red', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss', fontsize = 20)\n",
    "plt.xlabel('Epoches', fontsize = 16)\n",
    "plt.ylabel('Loss (Binary cross entropy)', fontsize = 16)\n",
    "plt.legend(fontsize = 14)\n",
    "plt.xticks(ticks=range(0, len(acc)+1, 2), fontsize = 12)\n",
    "plt.yticks(fontsize = 12)\n",
    "\n",
    "# plt.suptitle('Pre-trained model: {}\\nLearning rate: {}'.format(model_name, lr),\n",
    "#             x = 0.1, y = 1, ha = 'left', fontsize = 14)\n",
    "plt.savefig('results/Loss and Accuracy {}.jpg'.format(ID))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imgs = os.listdir('test/')\n",
    "ids = []\n",
    "label = []\n",
    "for i in tqdm(test_imgs):\n",
    "    img = image.load_img('test/' + i, target_size=(im_width, im_height))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    label.append(model.predict(x)[0][0])\n",
    "    ids.append(int(i.split('.')[0]))\n",
    "sub = pd.DataFrame({'id': ids, 'label': label}).sort_values('id',axis = 0, ascending = True)\n",
    "sub.to_csv('results/submission {}.csv'.format(ID), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[20, 6])\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(sub['id'], sub['label'], alpha=0.5);\n",
    "plt.title('A', fontsize = 16)\n",
    "plt.xlabel('Image id', fontsize = 16)\n",
    "plt.ylabel('Probability', fontsize = 16)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(sub['label'], bins=10)\n",
    "plt.title('B', fontsize = 16)\n",
    "plt.xlabel('Image id', fontsize = 16)\n",
    "plt.ylabel('Probability', fontsize = 16)\n",
    "\n",
    "plt.suptitle('Distribution of predictions', fontsize = 20)\n",
    "\n",
    "plt.savefig('results/distribution_of_predictions_0.jpg', bbox_inches = 'tight');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/history {}.txt'.format(ID), 'wb') as his:\n",
    "    pickle.dump(history, his)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imgs = os.listdir(data_dir.format('test'))\n",
    "img_path = data_dir.format('test') + test_imgs[3]\n",
    "# img_path = './test04.jpg'\n",
    "img = image.load_img(img_path, target_size=(im_width, im_height))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "\n",
    "preds = model.predict(x)[0][0]\n",
    "print(preds)\n",
    "img_show = mpimg.imread(img_path)\n",
    "plt.imshow(img_show)\n",
    "plt.suptitle('This is a {}'.format('dog' if preds > 0.5 else 'cat'))\n",
    "plt.title('probability: {}%'.format(round(preds*100, 2)) if preds > 0.5 else round((1-preds)*100, 2));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('jupyter nbconvert Capstone.ipynb --to html --output results/Capstone_{}'.format(ID))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
