{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import random\n",
    "import os, shutil\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "from keras_applications.resnet import ResNet152\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "from keras.layers import GlobalAveragePooling2D, Dense, Dropout, Flatten,Input, concatenate\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras import backend, layers, models,utils\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras_efficientnets.efficientnet import EfficientNetB3\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "from sklearn.model_selection import train_test_split\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为每个版本设编号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预处理\n",
    "\n",
    "去除异常值并检查样本分布。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ex_files.txt', 'rb') as ef:\n",
    "    ex_files = pickle.load(ef)\n",
    "data_dir = './{}/'\n",
    "files = {x: os.listdir(data_dir.format(x)) for x in ['train', 'test']}\n",
    "clean_data = [i for i in files['train'] if i not in ex_files['resnet50']]\n",
    "clean_labels = [x.split('.')[0] for x in clean_data]\n",
    "df_smpl = pd.DataFrame({i: clean_labels.count(i) for i in set(clean_labels)}, index=[0])\n",
    "\n",
    "plt.pie(df_smpl, labels = df_smpl.columns, startangle=90, autopct = '%3.1f%%', textprops={'fontsize':16});\n",
    "plt.title('Sample size distribution', fontsize = 20)\n",
    "plt.savefig('sample_size_distribution_train.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "切分训练集和验证集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './train/'\n",
    "test_size = 0.2\n",
    "\n",
    "with open('data_set.txt', 'rb') as ds:\n",
    "    data_set = pickle.load(ds)\n",
    "file_df = pd.DataFrame({'file_names': clean_data, 'labels': [x.split('.')[0] for x in clean_data]})\n",
    "file_train, file_valid, label_train, label_valid = train_test_split(file_df['file_names'], file_df['labels'],\n",
    "                                                                    test_size = test_size, random_state = 0)\n",
    "file_valid = file_valid.tolist()\n",
    "file_train = file_train.tolist()\n",
    "print(len(file_train), len(file_valid), len(label_train), len(label_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = {'file_train': file_train, 'label_train': label_train, 'file_valid': file_valid, 'label_valid': label_valid}\n",
    "with open('data_set.txt', 'wb') as ds:\n",
    "    pickle.dump(data_set, ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "检查训练集和验证集样本分布。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[20, 6])\n",
    "plt.subplot(1, 2, 1)\n",
    "train_dist = pd.DataFrame(Counter(label_train), index=[0])\n",
    "ax, ltext, ptext = plt.pie(train_dist, labels = train_dist.columns, startangle=90, autopct = '%3.1f%%', textprops={'fontsize':16});\n",
    "plt.title('Sample size distribution of training set', fontsize = 20)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "valid_dist = pd.DataFrame(Counter(label_valid), index=[0])\n",
    "plt.pie(valid_dist, labels = valid_dist.columns, startangle=90, autopct = '%3.1f%%', textprops={'fontsize':16})\n",
    "plt.title('Sample size distribution of validation set', fontsize = 20);\n",
    "\n",
    "plt.savefig('sample_size_distribution_train_valid.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataf = {x: pd.DataFrame({'X': data_set['file_{}'.format(x)],\n",
    "                          'Y'.format(x): data_set['label_{}'.format(x)]}) for x in ['train', 'valid']}\n",
    "dataf['train'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据增强。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = {x: image.ImageDataGenerator(preprocessing_function=preprocess_input) for x in dataf.keys()}\n",
    "# datagen['train'].horizontal_flip = True\n",
    "# datagen['train'].rotation_range = 30\n",
    "# datagen['train'].zoom_range = 0.2\n",
    "# datagen['train'].vertical_flip = True\n",
    "# datagen['train'].zoom_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_width, im_height = 224, 224\n",
    "batch_size = 128\n",
    "\n",
    "generator = {x: datagen[x].flow_from_dataframe(\n",
    "    dataf[x], x_col = 'X', y_col = 'Y',directory = data_dir.format('train'),\n",
    "    target_size=(im_width, im_height),\n",
    "    batch_size=batch_size,\n",
    "    seed = 123,\n",
    "    class_mode = 'binary'\n",
    ") for x in datagen.keys()}\n",
    "\n",
    "generator['valid'].shuffle = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'ResNet152'\n",
    "model_base = ResNet152(weights = 'imagenet', include_top = False, input_shape = (im_width, im_height, 3),\n",
    "                           backend = backend, layers = layers, models = models, utils = utils)\n",
    "model = Sequential()\n",
    "model.add(model_base)\n",
    "model.add(GlobalAveragePooling2D())\n",
    "# model.add(Dropout(0.75))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of trainable weights befor freezing the model_base:', len(model.trainable_weights))\n",
    "model_base.trainable = False\n",
    "print('Number of trainable weights after freezing the model_base:', len(model.trainable_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.00005\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "model.compile(loss = \"binary_crossentropy\", optimizer = optimizers.Adam(lr=lr), metrics=[\"accuracy\"])\n",
    "\n",
    "epochs = 40\n",
    "history = model.fit_generator(generator['train'],\n",
    "                              steps_per_epoch=math.ceil(generator['train'].samples / batch_size),\n",
    "                              epochs=epochs,\n",
    "                              validation_data=generator['valid'],\n",
    "                              validation_steps=math.ceil(generator['valid'].samples / batch_size),\n",
    "                              callbacks=[early_stopping],\n",
    "                              verbose = 2)\n",
    "model.save('models/model_{}.h5'.format(ID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "x_axis = range(1, len(acc)+1)\n",
    "\n",
    "plt.figure(figsize = [20, 6])\n",
    "plt.subplots_adjust(wspace=0.3)\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(x_axis, acc, 'blue', label='Training Accurarcy')\n",
    "plt.plot(x_axis, val_acc, 'red', label='Validation Accurarcy')\n",
    "plt.title('Training and Validation Accurarcy', fontsize = 20)\n",
    "plt.xlabel('Epoches', fontsize = 16)\n",
    "plt.ylabel('Accuracy', fontsize = 16)\n",
    "plt.legend(fontsize = 14)\n",
    "plt.xticks(ticks=range(0, len(acc)+1, 2),fontsize = 12)\n",
    "plt.yticks(fontsize = 12)\n",
    "\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(x_axis, loss, 'blue', label='Training Loss')\n",
    "plt.plot(x_axis, val_loss, 'red', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss', fontsize = 20)\n",
    "plt.xlabel('Epoches', fontsize = 16)\n",
    "plt.ylabel('Loss (Binary cross entropy)', fontsize = 16)\n",
    "plt.legend(fontsize = 14)\n",
    "plt.xticks(ticks=range(0, len(acc)+1, 2), fontsize = 12)\n",
    "plt.yticks(fontsize = 12)\n",
    "\n",
    "# plt.suptitle('Pre-trained model: {}\\nLearning rate: {}'.format(model_name, lr),\n",
    "#             x = 0.1, y = 1, ha = 'left', fontsize = 14)\n",
    "plt.savefig('results/Loss and Accuracy {}.jpg'.format(ID))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imgs = os.listdir('test/')\n",
    "ids = []\n",
    "label = []\n",
    "for i in tqdm(test_imgs):\n",
    "    img = image.load_img('test/' + i, target_size=(im_width, im_height))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    label.append(model.predict(x)[0][0])\n",
    "    ids.append(int(i.split('.')[0]))\n",
    "sub = pd.DataFrame({'id': ids, 'label': label}).sort_values('id',axis = 0, ascending = True)\n",
    "sub.to_csv('results/submission {}.csv'.format(ID), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/history {}.txt'.format(ID), 'wb') as his:\n",
    "    pickle.dump(history, his)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imgs = os.listdir(data_dir.format('test'))\n",
    "img_path = data_dir.format('test') + test_imgs[3]\n",
    "# img_path = './test04.jpg'\n",
    "img = image.load_img(img_path, target_size=(im_width, im_height))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "\n",
    "preds = model.predict(x)[0][0]\n",
    "print(preds)\n",
    "img_show = mpimg.imread(img_path)\n",
    "plt.imshow(img_show)\n",
    "plt.suptitle('This is a {}'.format('dog' if preds > 0.5 else 'cat'))\n",
    "plt.title('probability: {}%'.format(round(preds*100, 2)) if preds > 0.5 else round((1-preds)*100, 2));"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
